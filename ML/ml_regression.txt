ml: select the model to train, manually perform feature extraction
dl: select the architecture of the network, features are automatic extracted by feeding in the training
data

least square: find the best fit curve, the sum of the squares of the offsets are used to estimate the best fit line

training set & testing set(for trained model)

visualize
sns.joinplot(x = 'Temperature', y = '', data = ,color = )
sns.lmplot(x = , y = , data = )

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)

X_train.shape
X_test.shape

from sklearn.linear_model import LinearRegression
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train, y_train)

regressor.coef_
regressor.intercept_

X_test.shape
y_predict = regressor.predit(X_test)

plt.scatter(X_train,y_train, color = 'gray')
plt.plot(X_train, regressor.predict(X_train), color = 'red')


MAE: average magnitude of error generated by the regression model
MSEï¼šdata outlier-more large, 

Poly
from sklearn.preprocessing import PolynomialFeatures
poly_regressor = PolynomialFeatures(degree = 2)
X_columns = poly_regressor.fit_transform(X_train)
X_columns
regressor = LinearRegression()
regressor.fit(X_columns, y_train)
y_predict = regressor.prefict(X_columns)
plt.scatter(X_train, y_train, color = )
ply.plot(X_train, y_predict, color = )
plt.ylabel()
plt.xlabel()
plt.title()


Multiple linear - examine relationship more than two variabels
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

stock = pd.read_csv("S&P.cvs")
stock.describe() - count/mean/std/max
stock.info()

sns.jointplot(x = , y = ,data = ) bivariant relationship
sns.pairplot()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

from sklearn.linear_model import LinearRegression
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train, y_train)

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from math import sqrt

RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict)), '.3f'))
MSE = mean_squared_error(y_test, y_predict)
MAE = mean_absolute_error(y_test, y_predict)
r2 = r2_score(y_test, y_predict)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)

i = 1
column_headers = df.coloumns.values
fig,ax = plt.subplot(2,4, figsize = (20, 20))
for column in column_headers:
  plt.subplot(2,4,i)
  sns.displot(admission[column]
  i+=1

plt.figure(figsize = (10, 10))
sns.heatmap(admission_df.corr(), annot = True)
sns.pairplot(df)

from sklean.metrics import r2_score, mean_absolute_error, mean_squared_error
from math import sqrt

k = X_test.shape[1]
n = len(X_test)
r2 = r2_score(y_test, y_predict)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)

y_predict = regressor.predict(X_test)

from mpl_toolkits.mplot3d import Axes3D
x_surf, y_surf = np.meshgrid(np.linspace(admission_df['GRE Score'].min(), admission_df['GRE Score'])
onlyX = pd.DataFrame({'GRE Score': x_surf.ravel(), 'TOEFL Score':y_surf.ravel()])
fittedY = regressor.predict(onlyX)
fittedY = fittedY.reshape(x_surf.shape)
ax.plot_surface(x_surf, y_surf, fittedY, color = 'r', alpha = 0.3)
