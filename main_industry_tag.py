# -*- coding: utf-8 -*-
"""main_industry_tag

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SI6woOsFxBZNvw-8LjrDrgAbQFC2qVWm
"""

# add your drive shortcut to your colab
from google.colab import drive
drive.mount('/content/drive')

# import packages
import sys
import time

import pickle
import numpy as np
import pandas as pd
from tqdm import tqdm

import nltk
import gensim
from textblob import Word
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer


nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')
STOPWORD_LIST = stopwords.words('english')

"""## Data Preprocessing

---
"""

# read data
default_path = r"/content/drive/MyDrive/FRE_GY_7043_Capstone_Project/Industry_Tags/"

def get_data(path):
    """
    Read data: From work directory, from parquet to pandas dataframe
    Clean data: Concatenate all the columns of dataframe to a single column dataframe
        which is of concatenated string data type

    :param path: string; default file path
    :return: pd.DataFrame; pandas dataframe wrt features and labels (target)
    """
    # target/label
    dv = pd.read_parquet(path + 'data_vendor.parquet').rename(columns={'target': 'target_dv'})
    es = pd.read_parquet(path + 'enterprise_software.parquet').rename(columns={'target': 'target_es'})
    ft = pd.read_parquet(path + 'fintech.parquet').rename(columns={'target': 'target_ft'})

    # feature/text
    wb = pd.read_parquet(path + 'WebsiteTxt.parquet').set_index('domain_name')

    # extract and concatenate content_txt & website summary since they have most information
    df = wb[["content_txt", "website_summary"]]
    # df[df.isna()] = ' '  # set None to space, avoid invalid string concat
    # df.loc[:, 'feature'] = df['domain_name']
    # df.set_index('domain_name', inplace=True)
    # for col in df.columns:
    #     df['feature'] = df['feature'].str.cat(df[col], sep=' ')

    # feature is a concatenated str of content_txt and website summary
    df = df.fillna(' ')
    df['feature'] = df['content_txt'].str.cat(df['website_summary'], sep=' ')

    return pd.concat([df, dv, es, ft], axis=1).dropna()

data = get_data(default_path)
data.head()

print(data.shape)
for col in data.columns[:3]:
    print(f"average length of text in {col} is {np.mean([len(x.split(' ')) for x in data[col]])}")

# preprocess data
def remove_special_characters(text):
    """
    Remove special characters
    Negative character set: [^xyz], [^\w\s]:
        any character which is mot a word or space will be replaced by ''
    :param text: pd.Series of str; text
    :return:
    """
    tqdm.pandas(desc=sys._getframe().f_code.co_name)
    return text.str.replace('[^\w\s]','')


def remove_common_words(text, top=10):
    """
    Remove common words
    Remove top 10 most frequent words
    :param text: pd.Series of str; text
    :param top: int; number of most frequent words we want to remove
    :return:
    """
    tqdm.pandas(desc=sys._getframe().f_code.co_name)
    freq = pd.Series(' '.join(text).split()).value_counts()[:top]
    print(f"top {top} common words: {freq}.")
    return text.progress_apply(lambda x: " ".join(x for x in x.split() if x not in freq.index))


def tokenise_text(text):
    """
    Tokenization by gensim simple preprocess
    This function removes punctuations, lowers cases, removes numbers
    :param text: pd.Series of str; text
    :return: generator
    """
    for sentence in tqdm(text, desc=sys._getframe().f_code.co_name):
        # deacc=True removes punctuations
        yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))


def remove_stopwords(text):
    """
    Remove stop words
    :param text: pd.Series of list; text
    :return:
    """
    tqdm.pandas(desc=sys._getframe().f_code.co_name)
    return text.progress_apply(lambda x: [w for w in x if w not in STOPWORD_LIST])


def lemmatise_text(text):
    """
    Lemmatise word in texts
    :param text: pd.Series of list; text
    :return:
    """
    tqdm.pandas(desc=sys._getframe().f_code.co_name)
    return text.progress_apply(lambda x: [Word(word).lemmatize() for word in x])


def normalise_text(text):
    """
    Integrated pipeline of text preprocessing
    :param text: pd.Series of str; text
    :return:
    """
    text = remove_special_characters(text)
    text = remove_common_words(text)
    text = pd.Series(tokenise_text(text))
    text = remove_stopwords(text)
    text = lemmatise_text(text)
    return text

# integrated text preprocessing pipeline
website_sum = normalise_text(data['website_summary'])
content_txt = normalise_text(data['content_txt'])
feature_cat = normalise_text(data['feature'])

data.loc[:, 'website_sum_token'] = website_sum.values
data.loc[:, 'content_txt_token'] = content_txt.values
data.loc[:, 'feature_cat_token'] = feature_cat.values
data.head()

data[['feature_cat_token']].head()

"""## Data Transformation

---

#### Doc2Vec Model
###### TODO: SPLIT TRAINING ON TRAIN AND TEST RESPECTIVELY
"""

def read_corpus(feature, label):
    """
    Read text and tag document with gensim.models.doc2vec.TaggedDocument
    :params df: pd.DataFrame; original dataframe
    :params feature_name: str; name of feature needed to be transformed
    :params target_name: str; name of target/label
    :return: generator;
    """
    for i in range(len(feature)):
        # if tokens_only:
        #     # for testing data,
        #     yield df[feature_name].iloc[i]
        # else:
        # for training data, add tags
        yield gensim.models.doc2vec.TaggedDocument(
            feature.iloc[i], list(label.iloc[i]))


def vectorise_text_dv(feature, label, feature_name='feature_cat_token'):
    """
    Vectorise whole documents (doc-vectors) instead of simple-averaging of Word2Vec vectors.
    reference: https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py

    :params df: pd.DataFrame; original dataframe
    :params feature_name: str; name of feature needed to be transformed
    :params target_name: str; name of target/label
    :return: np.array;
    """
    start_time = time.time()

    corpus = list(read_corpus(feature[feature_name], label))
    # test_corpus = list(test_feature[feature_name])

    # train Doc2Vec model on training data set
    model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=40)
    model.build_vocab(corpus)  # build vocabulary before training
    model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)

    features= []
    for i in range(len(corpus)):
        features.append(model.infer_vector(corpus[i].words))

    # for j in range(len(test_corpus)):
    #     test_features.append(model.infer_vector(test_corpus[j]))

    print(f"Doc2vec training time: {time.time() - start_time} seconds")

    return model, np.array(features)

# train the Doc2Vec model and transform text to vectors
# train_x, test_x, train_y, test_y = prepare_datasets(data[['website_sum_token', 'content_txt_token', 'feature_cat_token']], data[['target_es', 'target_dv', 'target_ft']], test_data_proportion=0.2)
model_dv, word_vector_dv = vectorise_text_dv(data, data[['target_es', 'target_dv', 'target_ft']])

word_vector_dv = pd.DataFrame(word_vector_dv, index=data.index)
print(data.shape[0], word_vector_dv.shape[0])
word_vector_dv.head()

word_vector_dv.to_csv(default_path + "word_vector_dv.csv")
# save dv model
pickle.dump(model_dv, open(default_path + "model_dv.pickle.dat", "wb"))

"""#### Word2Vec Model"""

def vectorise_text_wv(feature, feature_name='feature_cat_token'):
    """
    Word2Vec Model
    reference: https://www.cnblogs.com/kjkj/p/9825418.html
    """
    start_time = time.time()
    model = gensim.models.Word2Vec(feature[feature_name], size=100, window=10, min_count=5, workers=10)
    model.train(feature[feature_name], total_examples=len(feature[feature_name]), epochs=10)

    print(f"Word2vec training time: {time.time() - start_time} seconds")
    return model


def average_word_vector(words, model, vocabulary, num_features):
    """
    Vectorize word and take the average of every word vector for a single obs.
    :param words: list;
    :param model: gensim.models.word2vec.Word2Vec; Word2Vec model
    :return: averaged word vector for a single obs
    """
    feature_vector = np.zeros((num_features,), dtype="float64")
    num_words = 0

    for word in words:
        if word in vocabulary:
            num_words = num_words + 1.
            feature_vector = np.add(feature_vector, model.wv[word])

    if num_words:
        feature_vector = np.divide(feature_vector, num_words)

    return feature_vector


def average_word_vectorizer(text, model, num_features):
    """
    average_word_vector process for a whole pd.Series.
    :params text: pd.Series / list;
        every obs in text is a tokenized_sentence
    :params model: gensim.models.word2vec.Word2Vec; Word2Vec model
    :return: np.array;
    """
    vocabulary = set(model.wv.index2word)
    features = [average_word_vector(obs, model, vocabulary, num_features)
                for obs in text]
    return np.array(features)

# train the Word2Vec model and transform text to vectors
model_wv = vectorise_text_wv(data)
avg_wv_feature = average_word_vectorizer(data['feature_cat_token'], model_wv, num_features=100)

word_vector_wv = pd.DataFrame(avg_wv_feature, index=data.index)
word_vector_wv.head()

# write Word2Vec word vector to csv file (900 * 23440), word2vec model to pickle
word_vector_wv.to_csv(default_path + "word_vector_wv.csv")
# save wv model
pickle.dump(model_wv, open(default_path + "model_wv.pickle.dat", "wb"))

"""#### CountVectorizer and TF-IDF Vectorizer"""

def prepare_datasets(corpus, labels, test_data_proportion=0.2):
    """
    train test split, default proportion of test data is 20%
    """
    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels, test_size=test_data_proportion, random_state=3)
    return train_X, test_X, train_Y, test_Y

def vectorise_bow(train_feature):
    vectoriser = CountVectorizer(min_df=1, ngram_range=(1, 1))  # unigram
    features = vectoriser.fit_transform(train_feature)
    return vectoriser, features


def vectorise_tfidf(bow_feature):
    transformer = TfidfTransformer(norm='l2', smooth_idf=True, use_idf=True)
    tfidf_matrix = transformer.fit_transform(bow_feature)
    return transformer, tfidf_matrix

train_x_str, test_x_str, train_y, test_y = prepare_datasets(data[['website_summary', 'content_txt', 'feature']], data[['target_es', 'target_dv', 'target_ft']], test_data_proportion=0.2)

# train bag of word model and transform text to word vector
bow_vectoriser, word_vector_bow_train = vectorise_bow(train_x_str['feature'])
word_vector_bow_test = bow_vectoriser.transform(test_x_str['feature'])

# print(word_vector_bow_train.todense())
# print(word_vector_bow_test.todense())

# train tf-idf model and transform text to word vector
tfidf_vectoriser, word_vector_tfidf_train = vectorise_tfidf(word_vector_bow_train)
word_vector_tfidf_test = tfidf_vectoriser.transform(word_vector_bow_test)

# print(word_vector_tfidf_train.todense())
# print(word_vector_tfidf_test.todense())

"""#### TfidfVectorizer"""

tfidf = TfidfVectorizer()
tfidf_vec = tfidf.fit_transform(data['feature_cat_token'].apply(' '.join))
print("n_samples: %d, n_features: %d" % tfidf.shape)

'''
Since the output of Count Vectorizer and TFIDF Vectorizer is csr_matrix, it is hard to display it.
You can treat it as matrix, whose index is same as the column 'domain_name' of DataFrame 'data'.
Then you can split it and use the code like the following to train it:

    naive_bayes_classifier.fit(tfidf, y)

Or you may transform it into dataframe, but it might be a little bit time-consuming and RAM-consuming.

    tfidf_df = pd.DataFrame(tfidf.toarray())

'''

"""## Individual Label Model

#### Resampling & Scaling
"""

# Import modules & Data
import random
import pandas as pd
import numpy as np

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier

from sklearn import preprocessing
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

from imblearn.over_sampling import SMOTE
from sklearn.neighbors import NearestNeighbors
from sklearn.utils import shuffle

default_path = r"/content/drive/MyDrive/FRE_GY_7043_Capstone_Project/Industry_Tags/"
data = pd.read_csv(default_path+'word_vector_dv.csv')
dv = pd.read_parquet(default_path+'data_vendor.parquet').rename(columns={'target': 'target_dv'})
es = pd.read_parquet(default_path+'enterprise_software.parquet').rename(columns={'target': 'target_es'})
ft = pd.read_parquet(default_path+'fintech.parquet').rename(columns={'target': 'target_ft'})

data = ((data.merge(dv, on = 'domain_name').dropna()).merge(es, on = 'domain_name').dropna()).merge(ft, on = 'domain_name').dropna()
X = data.loc[:,~data.columns.isin(['domain_name','target_dv','target_es','target_ft'])]
y = data[['target_dv','target_es','target_ft']]

# Define Resampling Function
# Define the MLSMOTE model to resample the highly imbalanced multi-label data
# Reference:https://medium.com/thecyphy/handling-data-imbalance-in-multi-label-classification-mlsmote-531155416b87
# Note: All existed imbalanced modules like SMOTE and ADASYN, etc. can't handle the multi-label data.

def get_tail_label(df):
    """
    Give tail label colums of the given target dataframe

    args
    df: pandas.DataFrame, target label df whose tail label has to identified

    return
    tail_label: list, a list containing column name of all the tail label
    """
    columns = df.columns
    n = len(columns)
    irpl = np.zeros(n)
    for column in range(n):
        irpl[column] = df[columns[column]].value_counts()[1]
    irpl = max(irpl)/irpl
    mir = np.average(irpl)
    tail_label = []
    for i in range(n):
        if irpl[i] > mir:
            tail_label.append(columns[i])
    return tail_label


def get_index(df):
  """
  give the index of all tail_label rows
  args
  df: pandas.DataFrame, target label df from which index for tail label has to identified

  return
  index: list, a list containing index number of all the tail label
  """
  tail_labels = get_tail_label(df)
  index = set()
  for tail_label in tail_labels:
    sub_index = set(df[df[tail_label]==1].index)
    index = index.union(sub_index)
  return list(index)


def get_minority_instance(X, y):
    """
    Give minority dataframe containing all the tail labels

    args
    X: pandas.DataFrame, the feature vector dataframe
    y: pandas.DataFrame, the target vector dataframe

    return
    X_sub: pandas.DataFrame, the feature vector minority dataframe
    y_sub: pandas.DataFrame, the target vector minority dataframe
    """
    index = get_index(y)
    X_sub = X[X.index.isin(index)].reset_index(drop = True)
    y_sub = y[y.index.isin(index)].reset_index(drop = True)
    return X_sub, y_sub


def nearest_neighbour(X):
    """
    Give index of 5 nearest neighbor of all the instance

    args
    X: np.array, array whose nearest neighbor has to find

    return
    indices: list of list, index of 5 NN of each element in X
    """
    nbs = NearestNeighbors(n_neighbors=5, metric='euclidean', algorithm='kd_tree').fit(X)
    euclidean, indices = nbs.kneighbors(X)
    return indices


def MLSMOTE(X, y, n_sample):
    """
    Give the augmented data using MLSMOTE algorithm

    args
    X: pandas.DataFrame, input vector DataFrame
    y: pandas.DataFrame, feature vector dataframe
    n_sample: int, number of newly generated sample

    return
    new_X: pandas.DataFrame, augmented feature vector data
    target: pandas.DataFrame, augmented target vector data
    """
    indices2 = nearest_neighbour(X)
    n = len(indices2)
    new_X = np.zeros((n_sample, X.shape[1]))
    target = np.zeros((n_sample, y.shape[1]))
    for i in range(n_sample):
        reference = random.randint(0,n-1)
        neighbour = random.choice(indices2[reference,1:])
        all_point = indices2[reference]
        nn_df = y[y.index.isin(all_point)]
        ser = nn_df.sum(axis = 0, skipna = True)
        target[i] = np.array([1 if val>2 else 0 for val in ser])
        ratio = random.random()
        gap = X.loc[reference,:] - X.loc[neighbour,:]
        new_X[i] = np.array(X.loc[reference,:] + ratio * gap)
    new_X = pd.DataFrame(new_X, columns=X.columns)
    target = pd.DataFrame(target, columns=y.columns)
    new_X = pd.concat([X, new_X], axis=0)
    target = pd.concat([y, target], axis=0)
    return new_X, target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)

print(len(y_train['target_dv'][y_train['target_dv'] == 1]))
print(len(y_train['target_dv'][y_train['target_dv'] == 0]))

print(len(y_train['target_es'][y_train['target_es'] == 1]))
print(len(y_train['target_es'][y_train['target_es'] == 0]))

print(len(y_train['target_ft'][y_train['target_ft'] == 1]))
print(len(y_train['target_ft'][y_train['target_ft'] == 0]))

# Resample train set
X_sub, y_sub = get_minority_instance(X_train, y_train)
X_res, y_res = MLSMOTE(X_sub, y_sub, 10000)

print(len(nearest_neighbour(X_sub)))
nearest_neighbour(X_sub)[:5]

# Combine train set with resample data
X_train = pd.concat([X_train,X_res])
y_train = pd.concat([y_train,y_res])

print(len(y_train['target_dv'][y_train['target_dv'] == 1]))
print(len(y_train['target_dv'][y_train['target_dv'] == 0]))

print(len(y_train['target_es'][y_train['target_es'] == 1]))
print(len(y_train['target_es'][y_train['target_es'] == 0]))

print(len(y_train['target_ft'][y_train['target_ft'] == 1]))
print(len(y_train['target_ft'][y_train['target_ft'] == 0]))

# Shuffle whole train set
tempdata = pd.concat([X_train, y_train], axis = 1)
tempdata = shuffle(tempdata)

# Get new train set and test set
X_train = tempdata.loc[:,~tempdata.columns.isin(['domain_name','target_dv','target_es','target_ft'])].to_numpy()
y_train = tempdata[['target_dv','target_es','target_ft']].to_numpy()
X_test = X_test.to_numpy()
y_test = y_test.to_numpy()

1888 + 18195

len(y_train)

# Scaler data
scaler = preprocessing.MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""#### Model Training

There are five common Binary Classification Models:
1.   Logistic Regression
2.   k-Nearest Neighbors
3.   Decision Trees
4.   Support Vector Machine
5.   Naive Bayes

But among all these five models, only Logistic Regression could estimate the exact possibility as required, which will be the score for label's prediction. Thus, we chose Logistic Regression as our model.

We also use GridSearchCV to chose the best parameters for each model. To save time for this main code, the GridSearchCV codes are shown in the GridSearchCV file in the same root_path.

Meanwhile, we also trained lots of NN models, but they didn't performs as well as Logistic Regression. Here are our best models:
"""

# Train data_vendor label Model
clf1 = LogisticRegression(C=10, solver = 'liblinear',max_iter=5000)
model1 = clf1.fit(X_train, y_train[:,0])
score1 = f1_score(model1.predict(X_test),y_test[:,0])

# Train enterprise_software label Model
clf2 = LogisticRegression(C=10, solver = 'newton-cg',max_iter=5000)
model2 = clf2.fit(X_train, y_train[:,1])
score2 = f1_score(model2.predict(X_test),y_test[:,1])

# Train fintech label Model
clf3 = LogisticRegression(C=10, solver = 'liblinear',max_iter=5000)
model3 = clf3.fit(X_train, y_train[:,2])
score3 = f1_score(model3.predict(X_test),y_test[:,2])

# Evaluate Models
print('f1 scores are: ', score1, score2, score3)
print('accuracies are: ', accuracy_score(model1.predict(X_test),y_test[:,0]), accuracy_score(model2.predict(X_test),y_test[:,1]),accuracy_score(model3.predict(X_test),y_test[:,2]))

"""#### Save Model & Scaler"""

import pickle   # Save Model for further use
filename_1 = 'indi_tag_dv.sav'
pickle.dump(model1, open(filename_1, 'wb'))
filename_2 = 'indi_tag_es.sav'
pickle.dump(model2, open(filename_2, 'wb'))
filename_3 = 'indi_tag_ft.sav'
pickle.dump(model3, open(filename_3, 'wb'))

import joblib # Save Scaler for further use
scaler_filename = "scaler.save"
joblib.dump(scaler, scaler_filename)

"""## Predict Websites' Labels: Score"""

# Predict websites labels and scores

X = X.to_numpy()
y = y.to_numpy()

clf1_pred = model1.predict(scaler.transform(X)) # Predict Labels 1 for all websites
clf2_pred = model2.predict(scaler.transform(X)) # Predict Labels 2 for all websites
clf3_pred = model3.predict(scaler.transform(X)) # Predict Labels 3 for all websites

clf1_p=clf1.predict_proba(scaler.transform(X))  # Predict score of Labels 1 for all websites
clf2_p=clf2.predict_proba(scaler.transform(X))  # Predict score of Labels 2 for all websites
clf3_p=clf3.predict_proba(scaler.transform(X))  # Predict score of Labels 3 for all websites

#Evaluate Models' Performance on all websites
print('f1 scores on X are: ', f1_score(clf1_pred,y[:,0]), f1_score(clf2_pred,y[:,1]),f1_score(clf3_pred,y[:,2]))
print('accuracies on X are: ', accuracy_score(clf1_pred,y[:,0]), accuracy_score(clf2_pred,y[:,1]),accuracy_score(clf3_pred,y[:,2]))

# Load data to DataFrame
tags = pd.DataFrame()
tags.loc[:,'domain_name'] = data['domain_name']
tags.loc[:,'data_vendor'] = clf1_pred
tags.loc[:,'data_vendor_score'] = (clf1_p[:,1]*100).astype(int)
tags.loc[:,'entrprise_software'] = clf2_pred
tags.loc[:,'entrprise_software_score'] = (clf2_p[:,1]*100).astype(int)
tags.loc[:,'fintech'] = clf3_pred
tags.loc[:,'fintech_score'] = (clf3_p[:,1]*100).astype(int)

# Change forms
tags.loc[tags.data_vendor == 0, 'data_vendor'] = ' '
tags.loc[tags.data_vendor == 1, 'data_vendor'] = 'data_vendor'
tags.loc[tags.entrprise_software == 0, 'entrprise_software'] = ' '
tags.loc[tags.entrprise_software == 1, 'entrprise_software'] = 'entrprise_software'
tags.loc[tags.fintech == 0, 'fintech'] = ' '
tags.loc[tags.fintech == 1, 'fintech'] = 'fintech'
tags

# Save DataFrame to csv and parquet
tags.to_csv('/content/drive/MyDrive/Colab Notebooks/individual_models_tags_result.csv')
tags.to_parquet('/content/drive/MyDrive/Colab Notebooks/individual_models_tags_result.parquet')
tags.to_csv('/content/drive/MyDrive/FRE_GY_7043_Capstone_Project/Industry_Tags/individual_models_tags_result.csv')
tags.to_parquet('/content/drive/MyDrive/FRE_GY_7043_Capstone_Project/Industry_Tags/individual_models_tags_result.parquet')

"""## Multi Label Model

#### KNN, Logistics, Random Forest, and Decision Tree models
"""

import pyarrow.parquet as pq
import sklearn
import warnings

from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.multiclass import OneVsRestClassifier

data = pd.read_csv(default_path + 'word_vector_dv.csv', index_col='domain_name')
data

target = pd.concat([pd.read_parquet(default_path + 'data_vendor.parquet'),
                    pd.read_parquet(default_path + 'enterprise_software.parquet'),
                    pd.read_parquet(default_path + 'fintech.parquet')], axis=1)
target.columns = ['dv', 'es', 'ft']
#data = data.merge(target, on = 'domain_name').dropna()
target

X = data.loc[target.index, :]
y = target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)

# Resample train set
X_sub, y_sub = get_minority_instance(X_train, y_train)
X_res,y_res = MLSMOTE(X_sub, y_sub, 1500)

# Combine train set with resample data
X_train = pd.concat([X_train,X_res])
y_train = pd.concat([y_train,y_res])

# Shuffle whole train set
tempdata = pd.concat([X_train, y_train], axis = 1)
tempdata = shuffle(tempdata)

# Get new train set and test set

X_train = tempdata.loc[:,~tempdata.columns.isin(['domain_name','dv','es','ft'])].to_numpy()
y_train = tempdata[['dv','es','ft']].to_numpy()
X_test = X_test.to_numpy()
y_test = y_test.to_numpy()


scaler = preprocessing.MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print('X train lenth: ' + str(len(X_train)))
print('y train lenth: ' + str(len(y_train)))
print('X test lenth: ' + str(len(X_test)))
print('y test lenth: ' + str(len(y_test)))

def training(model):
    model.fit(X_train, y_train)
    pred_test = model.predict(X_test)
    f1_test = f1_score(y_test, pred_test, average=None)
    acc_test = accuracy_score(y_test, pred_test)
    pred_full = model.predict(scaler.transform(X))
    f1_full = f1_score(y, pred_full, average=None)
    acc_full = accuracy_score(y, pred_full)
    return f1_test, acc_test, f1_full, acc_full

LR = OneVsRestClassifier(LogisticRegression())
DT = DecisionTreeClassifier()
KNN = KNeighborsClassifier ()
RF = RandomForestClassifier()

dict_clf = {
    'LogisticRegression': LR,
    'DecisionTreeClassifier':DT,
    'KNeighborsClassifier':KNN,
    'RandomForestClassifier':RF
}

dicts = {
    'clf_name':[],
    'test_f1':[],
    'test_accuracy':[],
    'full_f1':[],
    'full_accuracy':[]
}

for name in dict_clf.keys():
    f1_test, acc_test, f1_full, acc_full = training(dict_clf[name])
    dicts['clf_name'].append(name)
    dicts['test_f1'].append(f1_test)
    dicts['test_accuracy'].append(acc_test)
    dicts['full_f1'].append(f1_full)
    dicts['full_accuracy'].append(acc_full)

results = pd.DataFrame(dicts)
results

"""Based on the results, Logistics Regression has the best performance

#### Find the best parameters for Logistics Regression Model
"""

LR = OneVsRestClassifier(LogisticRegression(penalty='l2', C=5, solver='lbfgs', max_iter=5000))
f1_test, acc_test, f1_full, acc_full = training(LR)

print('test f1 socre: ' + str(f1_test))
print('test accuracy socre: '+ str(acc_test))
print('full f1 socre: '+ str(f1_full))
print('full accuracy socre: '+ str(acc_full))

"""### Cross Validation for Logistic Regression"""

from sklearn.model_selection import cross_val_score

cross_val_avg = 0 # average score of cross validation
k = 5             # 5-fold cross validation

def compute_cross_val_avg(model, X, y, k):
    return cross_val_score(model, X.values, y.values, cv=k)


cross_val_avg = compute_cross_val_avg(regr, X_train, y_train, 5)

s = "Avg cross val score = {:.3f} {:.3f} {:.3f} {:.3f} {:.3f}".format(*cross_val_avg)
s









"""GridSearchCV result is on the LR_GridSearch.csv

Probability
"""

prob = LR.predict_proba(scaler.transform(X))
probility = target.copy()
probility['dv_probility'] = prob[:,0]*100
probility['es_probility'] = prob[:,1]*100
probility['ft_probility'] = prob[:,2]*100
probility['dv_probility'] = probility['dv_probility'].astype(int)
probility['es_probility'] = probility['es_probility'].astype(int)
probility['ft_probility'] = probility['ft_probility'].astype(int)
probility

probility.to_parquet(default_path + 'Multilabels_LR_result.parquet')

probility.to_csv(default_path + 'Multilabels_LR_result.csv')

"""## Neural Network"""

pip install keras_tuner

import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from keras.callbacks import EarlyStopping
import keras.backend as K
import keras_tuner

# Read the data from csv file and arrange the multi-label target array
word_vector = pd.read_csv(default_path + "word_vector_dv.csv", index_col='domain_name')
target = pd.concat([pd.read_parquet(default_path + 'enterprise_software.parquet'),
                    pd.read_parquet(default_path + 'data_vendor.parquet'),
                    pd.read_parquet(default_path + 'fintech.parquet')], axis=1)
target.columns = ['es', 'dv', 'ft']

X = word_vector.loc[target.index, :]
y = target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)

# Resample train set
X_sub, y_sub = get_minority_instance(X_train, y_train)
X_res, y_res = MLSMOTE(X_sub, y_sub, 1500)

# Combine train set with resample data
X_train = pd.concat([X_train, X_res])
y_train = pd.concat([y_train, y_res])

# Shuffle whole train set
tempdata = pd.concat([X_train, y_train], axis=1)
tempdata = shuffle(tempdata)

# Get new train set and test set
X_train = tempdata.loc[:, ~tempdata.columns.isin(['domain_name','dv','es','ft'])]
y_train = tempdata[['es', 'dv', 'ft']]

print('X train lenth: ' + str(len(X_train)))
print('y train lenth: ' + str(len(y_train)))
print('X test lenth: ' + str(len(X_test)))
print('y test lenth: ' + str(len(y_test)))

"""#### MLP"""

#taken from old keras source code
def get_f1(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2 * (precision * recall)/(precision + recall + K.epsilon())
    return f1_val


# We try a simple MLP model first
def get_model(n_inputs, n_outputs):
    model = Sequential()
    # model.add(LSTM(units=100, return_sequences=True, input_shape=(n_inputs, n_inputs)))
    # model.add(LSTM(units=100))
    model.add(Dense(100, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))
    model.add(Dense(120, activation='relu'))
    model.add(Dense(100, activation='relu'))
    model.add(Dense(n_outputs, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[get_f1])
    return model

model = get_model(1000, 3)
model.fit(X_train, y_train, verbose=1, epochs=10, validation_split=0.2)

print('The model loss on the test data is: ', model.evaluate(X_test, y_test))
print('The model accuracy for all the label together is: ', accuracy_score(y_test, model.predict(X_test).round()))
print('The model f1 score for all the label seperately is: ', f1_score(y_test, model.predict(X_test).round(), average=None))

"""#### Model Tuning"""

# Now we have a simple model which performs not bad,
# we should implement the hyperparametre tuning to improve the model preformance.
def build_model(hp):
    # Build a model function to define our search space
    model = Sequential()
    model.add(Flatten())
    # Tune the number of layers.
    for i in range(hp.Int("num_layers", 1, 5)):
        model.add(
            Dense(
                # Tune number of units separately.
                units=hp.Int(f"units_{i}", min_value=12, max_value=100, step=8),
                activation=hp.Choice("activation", ["relu", "tanh"]),
            )
        )
    model.add(Dense(3, activation="sigmoid")) # Multi-label output layer
    learning_rate = hp.Float("lr", min_value=1e-4, max_value=1e-2, sampling="log") # Tune the learning rate
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss="binary_crossentropy", # Multi-label loss function
        metrics=[get_f1]
    )
    return model

build_model(keras_tuner.HyperParameters())

# Use RandomSearch from keras_tuner to search the best parametre
tuner = keras_tuner.RandomSearch(
    hypermodel=build_model,
    objective="val_loss",
    max_trials=20,
    executions_per_trial=5,
    overwrite = True
)
tuner.search_space_summary() # Show the searching space

# Start Searching
tuner.search(X_train, y_train, epochs=3, validation_split=0.2)

# Show the best result
tuner.results_summary(1)

# Get the top one hyperparameters.
best_hps = tuner.get_best_hyperparameters(1)
# Build the model with the best hp.
best_model = build_model(best_hps[0])

monitor_val_loss = EarlyStopping(monitor='val_loss', patience=5)
best_model.fit(X_train, y_train, batch_size=1200, epochs=30, validation_split=0.2,
          callbacks=[monitor_val_loss], )

# Show the best model result on the test set
print('The best model loss on the test data is: ', best_model.evaluate(X_test, y_test))
print('The best model accuracy for all the label together is: ', accuracy_score(y_test, best_model.predict(X_test).round()))
print('The best model f1 score for all the label seperately is: ', f1_score(y_test, best_model.predict(X_test).round(), average=None))

# Output the score of all the companies
score_MLP = pd.DataFrame((best_model.predict(X) * 100).round(), index=target.index)
score_MLP.columns = ['enterprise_software', 'data_vendor', 'fintech']

score_MLP.head(10)

# score = pd.DataFrame()
# for column in score_MLP.columns:
#   temp = pd.DataFrame(score_MLP[column]).rename(columns={column: 'industry_score'})
#   temp['industry'] = column
#   temp = temp[['industry', 'industry_score']]
#   score = pd.concat([score, temp], axis=0)

score_MLP.to_parquet(default_path + "score_MLP.parquet")
score_MLP.to_csv(default_path + "score_MLP.csv")

tag_nn = pd.read_parquet(default_path + 'tag_nn.parquet')
tag_nn

for col in ['data_vendor', 'enterprise_software', 'fintech']:
    tag_nn[col] = [1 if x == col else 0 for x in tag_nn[col]]

tag_nn

tag_nn.to_parquet(default_path + 'tag_nn.parquet')


